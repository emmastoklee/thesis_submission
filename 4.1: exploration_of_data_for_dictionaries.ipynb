{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "### Exploring the dataset to develop human-coded dictionaries\n",
        "Here I used word2vec to further develop my human-coded dictionaries for analysis of the topics discussed in articles."
      ],
      "metadata": {
        "id": "R1SJRZNMBpv0"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QI8NAfqmbcUH"
      },
      "outputs": [],
      "source": [
        "import gensim.downloader\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "import random\n",
        "from nltk.tokenize import word_tokenize\n",
        "from sklearn.manifold import TSNE\n",
        "from sklearn.metrics import accuracy_score\n",
        "import datasets\n",
        "import matplotlib.pyplot as plt\n",
        "import re\n",
        "from nltk.corpus import stopwords\n",
        "import nltk\n",
        "\n",
        "# enabling inline plots in Jupyter\n",
        "%matplotlib inline\n",
        "datasets.logging.set_verbosity_error()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install datasets"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gR5BSuvCrHyW",
        "outputId": "4fe0ed44-210a-41e3-e86e-37c0cf41bd48"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting datasets\n",
            "  Downloading datasets-3.1.0-py3-none-any.whl.metadata (20 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets) (3.16.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.26.4)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (18.0.0)\n",
            "Collecting dill<0.3.9,>=0.3.0 (from datasets)\n",
            "  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.66.6)\n",
            "Collecting xxhash (from datasets)\n",
            "  Downloading xxhash-3.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
            "Collecting multiprocess<0.70.17 (from datasets)\n",
            "  Downloading multiprocess-0.70.16-py310-none-any.whl.metadata (7.2 kB)\n",
            "Collecting fsspec<=2024.9.0,>=2023.1.0 (from fsspec[http]<=2024.9.0,>=2023.1.0->datasets)\n",
            "  Downloading fsspec-2024.9.0-py3-none-any.whl.metadata (11 kB)\n",
            "Collecting aiohttp (from datasets)\n",
            "  Downloading aiohttp-3.10.10-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.6 kB)\n",
            "Requirement already satisfied: huggingface-hub>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.24.7)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0.2)\n",
            "Collecting aiohappyeyeballs>=2.3.0 (from aiohttp->datasets)\n",
            "  Downloading aiohappyeyeballs-2.4.3-py3-none-any.whl.metadata (6.1 kB)\n",
            "Collecting aiosignal>=1.1.2 (from aiohttp->datasets)\n",
            "  Downloading aiosignal-1.3.1-py3-none-any.whl.metadata (4.0 kB)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (24.2.0)\n",
            "Collecting frozenlist>=1.1.1 (from aiohttp->datasets)\n",
            "  Downloading frozenlist-1.5.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (13 kB)\n",
            "Collecting multidict<7.0,>=4.5 (from aiohttp->datasets)\n",
            "  Downloading multidict-6.1.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.0 kB)\n",
            "Collecting yarl<2.0,>=1.12.0 (from aiohttp->datasets)\n",
            "  Downloading yarl-1.17.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (64 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m64.8/64.8 kB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting async-timeout<5.0,>=4.0 (from aiohttp->datasets)\n",
            "  Downloading async_timeout-4.0.3-py3-none-any.whl.metadata (4.2 kB)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.23.0->datasets) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2024.8.30)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
            "Collecting propcache>=0.2.0 (from yarl<2.0,>=1.12.0->aiohttp->datasets)\n",
            "  Downloading propcache-0.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.7 kB)\n",
            "Downloading datasets-3.1.0-py3-none-any.whl (480 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m480.6/480.6 kB\u001b[0m \u001b[31m11.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading fsspec-2024.9.0-py3-none-any.whl (179 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m179.3/179.3 kB\u001b[0m \u001b[31m10.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading aiohttp-3.10.10-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m33.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading multiprocess-0.70.16-py310-none-any.whl (134 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading xxhash-3.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m12.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading aiohappyeyeballs-2.4.3-py3-none-any.whl (14 kB)\n",
            "Downloading aiosignal-1.3.1-py3-none-any.whl (7.6 kB)\n",
            "Downloading async_timeout-4.0.3-py3-none-any.whl (5.7 kB)\n",
            "Downloading frozenlist-1.5.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (241 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m241.9/241.9 kB\u001b[0m \u001b[31m17.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading multidict-6.1.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (124 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m124.6/124.6 kB\u001b[0m \u001b[31m10.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading yarl-1.17.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (318 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m318.7/318.7 kB\u001b[0m \u001b[31m23.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading propcache-0.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (208 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m208.9/208.9 kB\u001b[0m \u001b[31m14.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: xxhash, propcache, multidict, fsspec, frozenlist, dill, async-timeout, aiohappyeyeballs, yarl, multiprocess, aiosignal, aiohttp, datasets\n",
            "  Attempting uninstall: fsspec\n",
            "    Found existing installation: fsspec 2024.10.0\n",
            "    Uninstalling fsspec-2024.10.0:\n",
            "      Successfully uninstalled fsspec-2024.10.0\n",
            "Successfully installed aiohappyeyeballs-2.4.3 aiohttp-3.10.10 aiosignal-1.3.1 async-timeout-4.0.3 datasets-3.1.0 dill-0.3.8 frozenlist-1.5.0 fsspec-2024.9.0 multidict-6.1.0 multiprocess-0.70.16 propcache-0.2.0 xxhash-3.5.0 yarl-1.17.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BQcO2jnHbnld",
        "outputId": "242b344b-0f99-4ad7-b359-30b682d609ec"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Hw9IPkc2bcUH"
      },
      "outputs": [],
      "source": [
        "# solution\n",
        "class MyDataLoader(object):\n",
        "    \"\"\"\n",
        "    A DataLoader class for reading and iterating over a corpus file.\n",
        "\n",
        "    Args:\n",
        "        filename (str): The name of the corpus file.\n",
        "    \"\"\"\n",
        "    # initialize the corpus object for a given filename\n",
        "    def __init__(self, filename):\n",
        "        self.corpus = filename\n",
        "\n",
        "    # we will need to define what counts as a \"chunk\" in this file, so when the\n",
        "    # Dataloader is loading (iterating over) the file and feeding it to the embedding\n",
        "    # model, it knows what to treat as one unit. Here, we (arbitrarily) say that one\n",
        "    # line in the file (corresponding to a paragraph) is one chunk.\n",
        "\n",
        "    def __iter__(self):\n",
        "    # _iter_ator function to iterate over the lines of the corpus file.\n",
        "        for line in open(self.corpus, \"r\", encoding=\"utf-8\"):\n",
        "            # checking that the line is not empty:\n",
        "            if line.strip():\n",
        "            # you may do some pre-processing on-the-fly. here we tokenize and lowercase\n",
        "            # the string before yielding it\n",
        "                line = word_tokenize(line)\n",
        "                line = [x.lower() for x in line]\n",
        "                yield line"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_xeykiTTbcUI"
      },
      "outputs": [],
      "source": [
        "liwc_analysis_men = pd.read_csv('/content/drive/MyDrive/SDS/Thesis/Data/liwc_analysis_men.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "crUJTHrabcUI"
      },
      "outputs": [],
      "source": [
        "liwc_analysis_women = pd.read_csv('/content/drive/MyDrive/SDS/Thesis/Data/liwc_analysis_women.csv')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Ensure nltk's tokenizer resources are available\n",
        "nltk.download('punkt')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DbwW7OC4rlzH",
        "outputId": "109e83dc-6c98-4bd0-8ace-dbe999165131"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vJFSY_EwbcUI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "785472a8-d316-4860-ad2e-ee4239be8509"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tokenized men text: [['with', 'all', 'eyes', 'on', 'diego', 'costa', 'and', 'where', 'he', 'stamps', 'his', 'feet', 'following', 'chelsea', 's', 'fractious', 'capital', 'cup', 'win', 'over', 'liverpool', 'the', 'other', 'striker', 'who', 'left', 'an', 'unfortunate', 'mark', 'at', 'stamford', 'bridge', 'was', 'relegated', 'to', 'a', 'footnote'], ['fitting', 'for', 'that', 'is', 'how', 'mario', 'balotelli', 's', 'liverpool', 'career', 'is', 'playing', 'out'], ['sky', 's', 'commentary', 'team', 'heralded', 'balotelli', 's', 'introduction', 'for', 'lazar', 'markovic', 'as', 'a', 'chance', 'to', 'put', 'one', 'over', 'the', 'former', 'internazionale', 'coach', 'who', 'labelled', 'him', 'unmanageable', 'josé', 'mourinho'], ['ultimately', 'his', 'contribution', 'amounted', 'to', 'no', 'more', 'than', 'weakening', 'a', 'previously', 'impressive', 'dangerous', 'liverpool', 'performance', 'and', 'unwittingly', 'assisting', 'in', 'chelsea', 's', 'winning', 'goal'], ['not', 'all', 'substitutions', 'succeed', 'and', 'not', 'all', 'the', 'blame', 'for', 'branislav', 'ivanovic', 's', 'header', 'belongs', 'to', 'balotelli']]\n",
            "Tokenized women text: [['man', 'of', 'the', 'day', 'chief', 'spartan', 'robbie', 'dale', 'came', 'so', 'close', 'to', 'undoing', 'birmingham'], ['it', 'drew', 'attention', 'in', 'unlikely', 'places', 'briefly', 'earning', 'a', 'follow', 'from', 'barackobama'], ['david', 'raya', 'martin', 'pulled', 'off', 'a', 'impression', 'in', 'goal', 'for', 'southport'], ['it', 'took', 'a', 'penalty', 'to', 'beat', 'him'], ['bullies', 'of', 'the', 'day', 'west', 'brom', 'laid', 'on', 'an', 'unromantic', 'mauling', 'of', 'gateshead', 'and', 'have', 'now', 'scored', 'goals', 'in', 'their', 'past', 'four', 'fa', 'cup', 'home', 'games', 'against', 'sides']]\n"
          ]
        }
      ],
      "source": [
        "# Sample text data\n",
        "men_text_raw = [str(text) for text in liwc_analysis_men['article_text'] if text is not None]\n",
        "women_text_raw = [str(text) for text in liwc_analysis_women['article_text'] if text is not None]\n",
        "\n",
        "# Function to split text into sentences and tokenize each sentence\n",
        "def process_text(raw_text):\n",
        "    sentences = []\n",
        "    for text in raw_text:\n",
        "        # Split text into sentences based on '.', '!', or '?' delimiters\n",
        "        split_sentences = re.split(r'[.!?]', text)\n",
        "\n",
        "        # Tokenize each sentence using word_tokenize (and keep stopwords)\n",
        "        tokenized_sentences = []\n",
        "        for sentence in split_sentences:\n",
        "            # Use nltk's word_tokenize and convert to lowercase\n",
        "            tokens = [word.lower() for word in word_tokenize(sentence) if word.isalpha()]  # Keep only alphabetic tokens\n",
        "            # Append the tokenized sentence if it's not empty\n",
        "            if tokens:\n",
        "                tokenized_sentences.append(tokens)\n",
        "\n",
        "        # Append each processed sentence to the sentences list\n",
        "        sentences.extend(tokenized_sentences)\n",
        "    return sentences\n",
        "\n",
        "# Process both men and women texts\n",
        "men_text = process_text(men_text_raw)\n",
        "women_text = process_text(women_text_raw)\n",
        "\n",
        "# Example output\n",
        "print(\"Tokenized men text:\", men_text[:5])  # Display first 5 tokenized sentences\n",
        "print(\"Tokenized women text:\", women_text[:5])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4aAFtmPWbcUI"
      },
      "outputs": [],
      "source": [
        "# training the Skip-Gram - model\n",
        "men2vec = gensim.models.Word2Vec(\n",
        "    men_text,   # the larger corpus object we've loaded\n",
        "    vector_size=300,     # the dimensionality of the target vectors\n",
        "    window=3,     # window ngram size\n",
        "    min_count=5,  # ignoring low-frequency words\n",
        "    epochs=3,      # how many training passes to have\n",
        "    sg = 1)       # 1 for skip-gram model, 0 for cbow\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eDbQGErPbcUI"
      },
      "outputs": [],
      "source": [
        "# training the Skip-Gram - model\n",
        "women2vec = gensim.models.Word2Vec(\n",
        "    women_text,   # the larger corpus object we've loaded\n",
        "    vector_size=300,     # the dimensionality of the target vectors\n",
        "    window=3,     # window ngram size\n",
        "    min_count=5,  # ignoring low-frequency words\n",
        "    epochs=3,      # how many training passes to have\n",
        "    sg = 1)       # 1 for skip-gram model, 0 for cbow\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# load in men2vec model\n",
        "men2vec = gensim.models.Word2Vec.load('/content/drive/MyDrive/SDS/Thesis/Data/men2vec.model')"
      ],
      "metadata": {
        "id": "BmNmZWzKqsai"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# load in women2vec model\n",
        "women2vec = gensim.models.Word2Vec.load('/content/drive/MyDrive/SDS/Thesis/Data/women2vec.model')"
      ],
      "metadata": {
        "id": "exGrQh9ZqxAE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o3xLMKRDbcUJ",
        "outputId": "037f63da-0230-4e82-a093-522d69f03217"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "'tournament' was mentioned 14939 time(s) in the corpus\n",
            "The words most similar to tournament are:\n",
            "[('tournaments', 0.6567858457565308), ('euros', 0.6496333479881287), ('confederations', 0.6176959276199341), ('euro', 0.6136319637298584), ('wc', 0.6122615337371826), ('world', 0.6076704263687134), ('jamboree', 0.5851637721061707), ('competition', 0.58104008436203), ('olympics', 0.5578311681747437), ('toulon', 0.5536025166511536)]\n",
            "'championship' was mentioned 16327 time(s) in the corpus\n",
            "The words most similar to championship are:\n",
            "[('spl', 0.600573718547821), ('premiership', 0.5939382910728455), ('tykes', 0.5684801936149597), ('prem', 0.5678666234016418), ('jpt', 0.5631271600723267), ('ipswich', 0.5630439519882202), ('tier', 0.5626885294914246), ('middlesbrough', 0.5596318244934082), ('premier', 0.5564159154891968), ('league', 0.5554913282394409)]\n",
            "'finals' was mentioned 4319 time(s) in the corpus\n",
            "The words most similar to finals are:\n",
            "[('championships', 0.685486912727356), ('tournaments', 0.6521379947662354), ('qualifiers', 0.6357651948928833), ('semifinal', 0.6352814435958862), ('knockouts', 0.6175617575645447), ('final', 0.6142684817314148), ('fairs', 0.6128658056259155), ('quarterfinals', 0.6125301718711853), ('trebles', 0.6114580631256104), ('semis', 0.6074410676956177)]\n"
          ]
        }
      ],
      "source": [
        "for word_of_interest in ['tournament', 'championship', 'finals']:\n",
        "    use_count = men2vec.wv.get_vecattr(word_of_interest, \"count\")\n",
        "    print(f\"'{word_of_interest}' was mentioned {use_count} time(s) in the corpus\")\n",
        "    print(f\"The words most similar to {word_of_interest} are:\")\n",
        "    print(men2vec.wv.most_similar(word_of_interest))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z-BZK7FBbcUJ",
        "outputId": "4cf6734d-83cb-45e8-bc27-b01f450bda7a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "'tournament' was mentioned 3464 time(s) in the corpus\n",
            "The words most similar to tournament are:\n",
            "[('euros', 0.7444405555725098), ('euro', 0.7153774499893188), ('competition', 0.7028447985649109), ('world', 0.6778019070625305), ('tokyo', 0.6638725996017456), ('olympics', 0.662897527217865), ('tournaments', 0.6528619527816772), ('milestone', 0.6435577273368835), ('shebelieves', 0.6323204636573792), ('earliest', 0.6282159090042114)]\n",
            "'championship' was mentioned 865 time(s) in the corpus\n",
            "The words most similar to championship are:\n",
            "[('championships', 0.8226462602615356), ('promotion', 0.7910805940628052), ('maiden', 0.7849583625793457), ('undefeated', 0.7748493552207947), ('inaugural', 0.7733708024024963), ('summit', 0.7721524834632874), ('destination', 0.7640843391418457), ('premiership', 0.7626152634620667), ('rescheduled', 0.7624508142471313), ('showpiece', 0.759446382522583)]\n",
            "'finals' was mentioned 390 time(s) in the corpus\n",
            "The words most similar to finals are:\n",
            "[('qualifiers', 0.8630778193473816), ('championships', 0.852670431137085), ('playoffs', 0.8397138714790344), ('semi', 0.8327130675315857), ('finalists', 0.8289790749549866), ('wcl', 0.8107743263244629), ('automatically', 0.8097190260887146), ('campaigns', 0.8067611455917358), ('semis', 0.806628942489624), ('rounds', 0.8050206303596497)]\n"
          ]
        }
      ],
      "source": [
        "for word_of_interest in ['tournament', 'championship', 'finals']:\n",
        "    use_count = women2vec.wv.get_vecattr(word_of_interest, \"count\")\n",
        "    print(f\"'{word_of_interest}' was mentioned {use_count} time(s) in the corpus\")\n",
        "    print(f\"The words most similar to {word_of_interest} are:\")\n",
        "    print(women2vec.wv.most_similar(word_of_interest))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# save model\n",
        "men2vec.save('/content/drive/MyDrive/SDS/Thesis/Data/men2vec.model')"
      ],
      "metadata": {
        "id": "2R59A2i0myDw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# save women model\n",
        "women2vec.save('/content/drive/MyDrive/SDS/Thesis/Data/women2vec.model')"
      ],
      "metadata": {
        "id": "xGUuvU7_04dn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "naOHCNJO068o"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.16"
    },
    "orig_nbformat": 4,
    "colab": {
      "provenance": [],
      "gpuType": "V28"
    },
    "accelerator": "TPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}